
import streamlit as st
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# --- CONFIGURAÃ‡ÃƒO DA PÃGINA ---
st.set_page_config(
    page_title="Konpeki",
    page_icon="ğŸ’",
    layout="centered",
    initial_sidebar_state="auto"
)

# --- LÃ“GICA DO CHAT ---

# FunÃ§Ã£o para gerar a resposta em streaming
def get_response_stream(user_prompt, chat_history):
    # Adiciona a pergunta do usuÃ¡rio ao histÃ³rico para enviar ao modelo
    messages = chat_history + [HumanMessage(content=user_prompt)]
    
    # Retorna o gerador de streaming
    return llm.stream(messages)

# Tenta inicializar o modelo de chat
try:
    llm = ChatOllama(model="llama3")
except Exception as e:
    st.error(f"Erro ao conectar com o Ollama: {e}")
    st.info("Certifique-se de que o Ollama estÃ¡ em execuÃ§Ã£o e o modelo 'llama3' foi baixado com 'ollama pull llama3'.")
    st.stop()

# --- INTERFACE DO USUÃRIO (UI) ---

st.title("Konpeki ğŸ’")

# Barra Lateral
with st.sidebar:
    st.header("OpÃ§Ãµes")
    if st.button("Limpar conversa com Konpeki"):
        # Reinicia o histÃ³rico, mantendo o prompt do sistema
        st.session_state.messages = [st.session_state.system_prompt, AIMessage(content="OlÃ¡! Eu sou Konpeki. Como posso te ajudar hoje?")]
        st.rerun()

# Define o prompt do sistema que dÃ¡ a personalidade Ã  IA
# Este prompt Ã© enviado ao modelo, mas nÃ£o Ã© exibido no chat
system_prompt = SystemMessage(content="VocÃª Ã© Konpeki, uma assistente de IA. Seu nome significa 'azul-celeste profundo' em japonÃªs. VocÃª Ã© prestativa, inteligente e formal. Sempre se apresente como Konpeki na sua primeira saudaÃ§Ã£o.")
st.session_state.system_prompt = system_prompt

# Inicializa o histÃ³rico da conversa se ainda nÃ£o existir
if "messages" not in st.session_state:
    st.session_state.messages = [system_prompt, AIMessage(content="OlÃ¡! Eu sou Konpeki. Como posso te ajudar hoje?")]

# Exibe as mensagens do histÃ³rico (pulando o prompt do sistema)
for message in st.session_state.messages:
    if isinstance(message, SystemMessage):
        continue
    avatar = "ğŸ’" if isinstance(message, AIMessage) else "ğŸ‘¤"
    with st.chat_message(message.type, avatar=avatar):
        st.markdown(message.content)

# Captura a entrada do usuÃ¡rio
if prompt := st.chat_input("Converse com a Konpeki..."):
    # Adiciona e exibe a mensagem do usuÃ¡rio
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    # Gera e exibe a resposta da IA em streaming
    with st.chat_message("ai", avatar="ğŸ’"):
        response_stream = get_response_stream(prompt, st.session_state.messages)
        full_response = st.write_stream(response_stream)
    
    # Adiciona a resposta completa da IA ao histÃ³rico
    st.session_state.messages.append(AIMessage(content=full_response))
